{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import math\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(alphas, timestep, x_shape):\n",
    "    device = timestep.device\n",
    "    out = torch.gather(alphas, index=timestep, dim=0).double().to(device)\n",
    "    return out.view([timestep.shape[0]] + (len(x_shape)-1) * [1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    betas   =   1 - alphas\n",
    "\n",
    "- betas:\n",
    "$$[\\beta_1,\\beta_2,...,\\beta_T]$$\n",
    "$$\\beta_t = 1 - \\alpha_t$$\n",
    "\n",
    "- alphas:\n",
    "$$ [\\alpha_1, \\alpha_2,...,\\alpha_T] $$\n",
    "\n",
    "- alphas_bar:\n",
    "$$ [\\alpha_1, \\alpha_1 \\alpha_2,..., \\alpha_1 \\alpha_2... \\alpha_T]$$\n",
    "$$[\\bar\\alpha_1, \\bar\\alpha_2,..., \\bar\\alpha_T]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- objective:\n",
    "$$\n",
    "   \\arg \\min_\\theta \\| \\epsilon - \\epsilon_\\theta \\| ^2\n",
    "$$\n",
    "- gradient:\n",
    "$$\n",
    "    \\nabla_\\theta \\| \\epsilon - \\epsilon_\\theta( \\sqrt{\\bar \\alpha_t} x_0 + \\sqrt{1 - \\bar \\alpha_t} \\epsilon , t) \\| ^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of the Unet:\n",
    "$$\\sqrt{\\bar \\alpha_t} x_0 + \\sqrt{1 - \\bar \\alpha_t} \\epsilon $$\n",
    "$$t$$  \n",
    "$$\\epsilon$$    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training(nn.Module):\n",
    "    def __init__(self, model, T, beta_1, beta_T):\n",
    "        super().__init__()\n",
    "        self.T = T\n",
    "        self.model = model\n",
    "\n",
    "        # 存储超参数到模型中\n",
    "        self.register_buffer(\n",
    "            'betas', torch.linspace(beta_1, beta_T, T).double()\n",
    "        )\n",
    "        alphas = 1. - self.betas\n",
    "        alphas_bar = torch.cumprod(alphas,dim=0)\n",
    "\n",
    "        self.register_buffer(\n",
    "            'sqrt_alphas_bar', torch.sqrt(alphas_bar)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            'sqrt_one_minus_alphas_bar', torch.sqrt(1. - alphas_bar)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x_0):\n",
    "        t = torch.randint(self.T, size=(x_0.shape[0],), device=x_0.device)\n",
    "        noise = torch.randn_like(x_0)\n",
    "        x_t = (\n",
    "              extract(self.sqrt_alphas_bar, t, x_0.shape) * x_0 + \n",
    "              extract(self.sqrt_one_minus_alphas_bar, t, x_0.shape) * noise)\n",
    "        \n",
    "        # reduction = 'none' 逐元素求mse\n",
    "        loss = F.mse_loss(\n",
    "            self.model(x_t, t), noise, reduction='none'\n",
    "        )\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- alphas_bar_prev:\n",
    "$$\\alpha_{t-1}$$\n",
    "$$ [1, \\alpha_1, \\alpha_1 \\alpha_2,..., \\alpha_1 \\alpha_2... \\alpha_{T-1}]$$\n",
    "$$[1, \\bar\\alpha_1, \\bar\\alpha_2,..., \\bar\\alpha_{T-1}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "objective:\n",
    "$$x_0$$\n",
    "\n",
    "- loop:\n",
    "$$\n",
    "x_{t-1} = \\frac {x_t}{\\sqrt \\alpha_t} - \\frac {1-\\alpha_t}{\\sqrt{\\alpha_t} \\sqrt{1- \\bar \\alpha_t} } \\epsilon_\\theta(x_t, t) + \\sigma_t z\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where:\n",
    "$$\n",
    "    \\sigma_t ^2 = \\frac {1-\\bar \\alpha_{t-1}}{1-\\bar \\alpha} \\beta_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(nn.Module):\n",
    "   def __init__(self, model, beta_1, beta_T, T):\n",
    "      super().__init__()\n",
    "\n",
    "      self.model = model\n",
    "      self.T = T\n",
    "\n",
    "      self.register_buffer('betas', torch.linspace(beta_1, beta_T, T).double())\n",
    "      alphas = 1. - self.betas\n",
    "      alphas_bar = torch.cumprod(alphas, dim=0)\n",
    "      # pad 参数是一个长度可变的列表或元组，遵循从最后一个维度向前的逆序填充规则\n",
    "      # [1, 0] 表示只在最后一个维度左侧填充 1 个元素，右侧不填充\n",
    "      alphas_bar_prev = F.pad(alphas_bar, pad=[1,0], value=1)[:T]\n",
    "\n",
    "      self.register_buffer('coeff1', 1./ torch.sqrt(alphas_bar))\n",
    "      self.register_buffer('coeff2', self.coeff1 * self.betas / torch.sqrt(1-alphas_bar))\n",
    "\n",
    "      self.register_buffer('posterior_var', self.betas * (1 - alphas_bar_prev) / (1 - alphas_bar))\n",
    "      \n",
    "   def predict_xt_prev_mean_from_eps(self, x_t, t, eps):\n",
    "      assert x_t.shape == eps.shape\n",
    "      mean = extract(self.coeff1, t, x_t.shape) * x_t - \\\n",
    "             extract(self.coeff2, t, x_t.shape) * eps\n",
    "      return mean\n",
    "   \n",
    "   def p_mean_variance(self, x_t, t):\n",
    "      var = extract(self.posterior_var, t, x_t.shape)\n",
    "      std = torch.sqrt(var)\n",
    "\n",
    "      eps = self.model(x_t, t)\n",
    "      xt_prev_mean = self.predict_xt_prev_mean_from_eps(x_t, t, eps)\n",
    "      return xt_prev_mean, std\n",
    "   \n",
    "   def forward(self, x_T):\n",
    "      x_t = x_T\n",
    "      for time_step in reversed(range(self.T)):\n",
    "         if time_step > 0:\n",
    "            noise = torch.randn_like(x_t, device=x_t.device)\n",
    "         else:\n",
    "            noise = torch.zeros_like(x_t, device=x_t.device)\n",
    "         t = x_t.new_ones([x_T.shape[0],],dtype=torch.long) * time_step\n",
    "         mean, std = self.p_mean_variance(x_t, t)\n",
    "         x_t = mean + noise * std\n",
    "         assert torch.isnan(x_t).int().sum() == 0, \"nan in tensor.\"\n",
    "      x_0 = x_t\n",
    "      return torch.clip(x_0,-1,1)\n",
    "         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
